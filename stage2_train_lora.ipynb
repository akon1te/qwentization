{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c550bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForSeq2Seq\n",
    ")\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ee2461",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"./models/Qwen3-8B-AWQ-MMLU\" \n",
    "output_dir = \"./models/Qwen3-8B-AWQ-MMLU-lora\"\n",
    "\n",
    "\n",
    "dataset = load_dataset(\"cais/mmlu\", \"all\", split=\"auxiliary_train\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "def format_and_tokenize(example):\n",
    "    options = [\"A\", \"B\", \"C\", \"D\"]\n",
    "    \n",
    "    prompt = f\"{example['question']}\\n\\nChoices:\\n\"\n",
    "    for i, choice in enumerate(example['choices']):\n",
    "        prompt += f\"{options[i]}. {choice}\\n\"\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    \n",
    "    response = f\" {options[example['answer']]}\"\n",
    "    \n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt},\n",
    "        {\"role\": \"assistant\", \"content\": response}\n",
    "    ]\n",
    "\n",
    "    full_text = tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "    \n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=False,\n",
    "        add_special_tokens=False\n",
    "    )\n",
    "    \n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    \n",
    "    return tokenized\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8adf9e7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:20<00:00, 484.48 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.select(range(10000))\n",
    "tokenized_dataset = dataset.map(format_and_tokenize, remove_columns=dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "68a26ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:01<00:00,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 43,646,976 || all params: 1,288,614,912 || trainable%: 3.3871\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    max_memory={0: \"22GiB\", 1: \"18GiB\"}, \n",
    ")\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc48f712",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    per_device_train_batch_size=6,\n",
    "    gradient_accumulation_steps=4,\n",
    "    num_train_epochs=1,\n",
    "    learning_rate=1e-5,\n",
    "    fp16=True,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=50,\n",
    "    optim=\"adamw_torch\",\n",
    "    report_to=\"tensorboard\",\n",
    "    remove_unused_columns=False,\n",
    "    \n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "14e270c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, pad_to_multiple_of=8, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "490f9746",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detected kernel version 5.4.210, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
      "  return data.pin_memory(device)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='209' max='209' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [209/209 1:31:56, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.942200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.702600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.576100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.405700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.316900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.240400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.199100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.135300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.118900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.062800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.069200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.057400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>2.054500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.043700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>2.047800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.991100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>1.997800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.050600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>2.020100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.992100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.pin_memory() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:46.)\n",
      "  return data.pin_memory(device)\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/torch/utils/data/_utils/pin_memory.py:57: DeprecationWarning: The argument 'device' of Tensor.is_pinned() is deprecated. Please do not pass this argument. (Triggered internally at /pytorch/aten/src/ATen/native/Memory.cpp:31.)\n",
      "  return data.pin_memory(device)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=209, training_loss=2.194456483758808, metrics={'train_runtime': 5544.0136, 'train_samples_per_second': 1.804, 'train_steps_per_second': 0.038, 'total_flos': 2.0185942625550336e+16, 'train_loss': 2.194456483758808, 'epoch': 1.0})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    data_collator=data_collator,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3d442638",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./models/Qwen3-8B-AWQ-MMLU-lora/tokenizer_config.json',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/special_tokens_map.json',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/chat_template.jinja',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/vocab.json',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/merges.txt',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/added_tokens.json',\n",
       " './models/Qwen3-8B-AWQ-MMLU-lora/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86ac1ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Files (1 / 1): 100%|██████████| 11.4MB / 11.4MB,  0.00B/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n",
      "Processing Files (1 / 1): 100%|██████████|  175MB /  175MB,  101MB/s  \n",
      "New Data Upload: |          |  0.00B /  0.00B,  0.00B/s  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/akon1te/qwen3-8b-awq-mmlu-lora/commit/4c3d9beccf8fce2d9211e26fa245b60ee7950927', commit_message='Upload model', commit_description='', oid='4c3d9beccf8fce2d9211e26fa245b60ee7950927', pr_url=None, repo_url=RepoUrl('https://huggingface.co/akon1te/qwen3-8b-awq-mmlu-lora', endpoint='https://huggingface.co', repo_type='model', repo_id='akon1te/qwen3-8b-awq-mmlu-lora'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub(\"akon1te/qwen3-8b-awq-mmlu-lora\")\n",
    "model.push_to_hub(\"akon1te/qwen3-8b-awq-mmlu-lora\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77d8c142",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "/home/timmiakov/internal_exps/qwentization/.venv/lib/python3.10/site-packages/awq/__init__.py:21: DeprecationWarning: \n",
      "I have left this message as the final dev message to help you transition.\n",
      "\n",
      "Important Notice:\n",
      "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
      "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
      "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
      "\n",
      "Alternative:\n",
      "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
      "\n",
      "For further inquiries, feel free to reach out:\n",
      "- X: https://x.com/casper_hansen_\n",
      "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
      "\n",
      "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:24<00:00, 42.22s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "\n",
    "base_model_path = \"akon1te/qwen3-8b-awq\"  # Путь к базовой AWQ модели\n",
    "adapter_path = \"akon1te/qwen3-8b-awq-mmlu-lora\"   # Путь к сохраненным адаптерам\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path, trust_remote_code=True)\n",
    "\n",
    "\n",
    "model = PeftModel.from_pretrained(\n",
    "    base_model,\n",
    "    adapter_path,\n",
    "    is_trainable=False # Мы только используем, не учим\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55c9c547",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Загружена dev выборка\n",
      "  Всего вопросов в dev выборке: 285\n",
      "  Количество предметов: 57\n",
      "Инициализация завершена. Эксперимент: awq_mmlu_dev_20251223_130308\n",
      "\n",
      "Эксперимент: awq_mmlu_dev_20251223_130308\n",
      "Модель: awq_mmlu\n",
      "Всего вопросов в dev: 285\n",
      "Количество предметов: 57\n",
      "Промпт стиль: zero-shot\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "57it [01:28,  1.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ОБЩАЯ ТОЧНОСТЬ: 0.7368 (73.68%)\n",
      "Правильных ответов: 210 из 285\n",
      "Оценено предметов: 57\n",
      "Пиковое потребление VRAM: 6161.36 MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from mmlu_benchmark import MMLUEvaluator\n",
    "\n",
    "evaluator = MMLUEvaluator(\n",
    "    model=model, tokenizer=tokenizer, device=\"cuda\",\n",
    "    split=\"dev\", per_subject_samples=10, seed=42, model_name=\"awq_mmlu\"\n",
    ")\n",
    "\n",
    "_ = evaluator.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a25933",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
